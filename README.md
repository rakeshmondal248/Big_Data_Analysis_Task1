# Big_Data_Analysis_Task1

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: RAKESH MONDAL

*INTERN ID*: CTIS5103

*DOMAIN*: DATA ANALYTICS

*DURATION*: 4 WEEKS

*MENTOR*: NEELA SANTOSH

*TASK DESCRIPTION*: The objective of this task is to perform big data analysis using PySpark, the Python interface for Apache Spark, and to understand how large-scale datasets can be processed efficiently using distributed computing. This task is part of an internship assignment aimed at building practical skills in big data technologies and real-world data analysis.
In today’s data-driven world, organizations generate massive amounts of data on a daily basis. Traditional data processing tools often struggle to handle such large volumes efficiently. Apache Spark addresses this challenge by providing a fast, scalable, and distributed computing framework. PySpark allows users to leverage Spark’s power using the Python programming language, making it easier to perform complex data processing and analytics tasks.
For this project, a sales dataset stored in CSV format is used as the input. The dataset contains information such as product names, categories, time periods, and sales values. The first step of the task involves creating a SparkSession, which acts as the entry point for all Spark-related operations. Once the SparkSession is initialized, the dataset is loaded into a Spark DataFrame. Spark DataFrames provide a structured and optimized way to handle large datasets, similar to tables in relational databases.
After loading the dataset, the schema is examined to understand the structure and data types of each column. This step is crucial because data collected from real-world sources often contains inconsistencies or incorrect data types. To ensure accurate analysis, data cleaning and transformation operations are performed. In this task, the sales column is converted into a numeric format to enable aggregation and mathematical computations.
Once the data is cleaned and prepared, various analytical operations are carried out using PySpark’s built-in functions. These operations include calculating the total sales across all records, analyzing sales performance based on product categories, and identifying top-performing products based on revenue. Grouping and aggregation functions such as groupBy and sum are used to efficiently compute these metrics across large datasets. These operations demonstrate how PySpark can process data in a distributed manner, significantly improving performance compared to traditional single-machine processing.
The task also highlights the importance of exploratory data analysis in understanding business performance. By analyzing category-wise and product-wise sales, meaningful insights can be derived that help in decision-making and performance evaluation. Such analysis is commonly used in real-world business scenarios, including retail analytics, financial reporting, and sales forecasting.
Overall, this task demonstrates the practical application of PySpark for big data analysis. It provides hands-on experience with data loading, cleaning, transformation, and aggregation using Apache Spark. The project showcases how large datasets can be processed efficiently and how valuable insights can be extracted using distributed computing techniques. Completing this task helps in developing a strong foundation in big data analytics and prepares learners for working with real-world large-scale data processing systems.

*OUTPUT*:

<img width="870" height="692" alt="Image" src="https://github.com/user-attachments/assets/8fee6a20-e319-4d1d-905d-76d9c5a9d8c8" />
